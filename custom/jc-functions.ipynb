{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report_cont(dataframe, list_of_features):\n",
    "    \"\"\"\n",
    "    Generates a data quality report for continuous features in a given DataFrame.\n",
    "\n",
    "    This function calculates and prints the following metrics for each feature in the provided list:\n",
    "    - Total count of non-missing values\n",
    "    - Total count of missing values\n",
    "    - Percentage of missing values\n",
    "    - Cardinality (number of unique values)\n",
    "    - Descriptive statistics (e.g., mean, standard deviation, min, max) for each feature\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The DataFrame containing the data to be analyzed.\n",
    "    list_of_features : list of str\n",
    "        The list of column names (features) for which the data quality report is generated.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        This function prints the data quality report to the console.\n",
    "\n",
    "    Docstring generated by ChatGPT.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    round_to = 2\n",
    "    list_feature_name = []\n",
    "    list_count = []\n",
    "    list_missing = []\n",
    "    list_percent = []\n",
    "    list_cardinality = []\n",
    "\n",
    "    for feature in list_of_features:\n",
    "        # Get stats for each feature\n",
    "        total_count = dataframe[feature].count()\n",
    "        total_missing = dataframe[feature].isnull().sum()\n",
    "        percent_missing = total_missing/total_count * 100\n",
    "        cardinality = len(dataframe[feature].unique())\n",
    "\n",
    "        # Append result to variables\n",
    "        list_feature_name.append(feature)\n",
    "        list_count.append(total_count)\n",
    "        list_missing.append(total_missing)\n",
    "        list_percent.append(np.round(percent_missing, round_to))\n",
    "        list_cardinality.append(cardinality)\n",
    "\n",
    "    # Create dataframe\n",
    "    data = {\n",
    "        \"Feature\": list_feature_name,\n",
    "        \"Count\": list_count,\n",
    "        \"Missing\": list_missing,\n",
    "        \"% missing\": list_percent,\n",
    "        \"Cardinality\": list_cardinality\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Get descriptive statistics and transpose\n",
    "    stats = np.round(dataframe[list_of_features].describe(), round_to)\n",
    "    transposed_stats = stats.T\n",
    "\n",
    "    # Print results\n",
    "    print(\"Data Quality for Continous Features\")\n",
    "    print(f'Total Features: {len(list_of_features)}')\n",
    "    print(df)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Descriptive Stats\")\n",
    "    print(transposed_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report_cat(dataframe, list_of_features):\n",
    "    \"\"\"\n",
    "    Generates a data quality report for categorical features in a given DataFrame.\n",
    "\n",
    "    This function calculates and prints the following metrics for each feature in the provided list:\n",
    "    - Total count of non-missing values\n",
    "    - Total count of missing values\n",
    "    - Percentage of missing values\n",
    "    - Cardinality (number of unique values)\n",
    "    - Mode 1 (most frequent value) and its frequency and percentage\n",
    "    - Mode 2 (second most frequent value) and its frequency and percentage\n",
    "    - Descriptive statistics for each feature\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe : pandas.DataFrame\n",
    "        The DataFrame containing the data to be analyzed.\n",
    "    list_of_features : list of str\n",
    "        The list of column names (features) for which the data quality report is generated.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        This function prints the data quality report to the console.\n",
    "    \n",
    "    Docstring generated by ChatGPT.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    round_to = 2\n",
    "    list_feature_name = []\n",
    "    list_count = []\n",
    "    list_missing = []\n",
    "    list_percent = []\n",
    "    list_cardinality = []\n",
    "    list_mode1 = []\n",
    "    list_mode1_freq = []\n",
    "    list_mode1_perc = []\n",
    "    list_mode2 = []\n",
    "    list_mode2_freq = []\n",
    "    list_mode2_perc = []\n",
    "\n",
    "    for feature in list_of_features:\n",
    "        \n",
    "        total_count = dataframe[feature].count()\n",
    "        total_missing = dataframe[feature].isnull().sum()\n",
    "        percent_missing = np.round(total_missing / total_count * 100, round_to)\n",
    "        cardinality = len(dataframe[feature].unique())\n",
    "\n",
    "        # Use value counts to get modes\n",
    "        results = dataframe[feature].value_counts()\n",
    "        # Calculate mode\n",
    "        mode1_name = results.index[0]\n",
    "        mode1_count = results.iloc[0]\n",
    "        mode1_percent = np.round((mode1_count / total_count) * 100, round_to)\n",
    "        # Calculate 2nd mode\n",
    "        mode2_name = results.index[1]\n",
    "        mode2_count = results.iloc[1]\n",
    "        mode2_percent = np.round((mode2_count / total_count) * 100, round_to)\n",
    "\n",
    "        # Append results to lists\n",
    "        list_feature_name.append(feature)\n",
    "        list_count.append(total_count)\n",
    "        list_missing.append(total_missing)\n",
    "        list_percent.append(percent_missing)\n",
    "        list_cardinality.append(cardinality)\n",
    "        list_mode1.append(mode1_name)\n",
    "        list_mode1_freq.append(mode1_count)\n",
    "        list_mode1_perc.append(mode1_percent)\n",
    "        list_mode2.append(mode2_name)\n",
    "        list_mode2_freq.append(mode2_count)\n",
    "        list_mode2_perc.append(mode2_percent)\n",
    "\n",
    "    # Create dataframes\n",
    "    data = {\n",
    "        \"Feature\": list_feature_name,\n",
    "        \"Count\": list_count,\n",
    "        \"Missing\": list_missing,\n",
    "        \"% Missing\": list_percent,\n",
    "        \"Cardinality\": list_cardinality,\n",
    "    }\n",
    "\n",
    "    data_mode1 = {\n",
    "        \"Feature\": list_feature_name,\n",
    "        \"Mode 1\": list_mode1,\n",
    "        \"Mode 1 Freq.\": list_mode1_freq,\n",
    "        \"Mode 1 %\": list_mode1_perc,\n",
    "    }\n",
    "\n",
    "    data_mode2 = {\n",
    "        \"Feature\": list_feature_name,\n",
    "        \"Mode 2\": list_mode2,\n",
    "        \"Mode 2 Freq.\": list_mode2_freq,\n",
    "        \"Mode 2 %\": list_mode2_perc,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df1 = pd.DataFrame(data_mode1)\n",
    "    df2 = pd.DataFrame(data_mode2)\n",
    "\n",
    "    # Get descriptive statistics and transpose\n",
    "    stats = dataframe[list_of_features].describe(include='object')\n",
    "    transposed_stats = stats.T\n",
    "\n",
    "    # Print results\n",
    "    print('Data Quality Report for Categorical Features')\n",
    "    print('============================================')\n",
    "    print('Stats')\n",
    "    print('-----')\n",
    "    print(df)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Mode 1')\n",
    "    print('------')\n",
    "    print(df1)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Mode 2')\n",
    "    print('------')\n",
    "    print(df2)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Descriptive Stats')\n",
    "    print('-----------------')\n",
    "    print(transposed_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_cm_metrics(cm, rnd=5):\n",
    "    \"\"\"\n",
    "    Calculate and return various performance metrics from a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix as a numpy array of any size.\n",
    "    rnd (int, optional): Number of decimal places to round the performance metrics. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A dataframe containing the calculated performance metrics:\n",
    "                      - 'Accuracy': Proportion of correct predictions.\n",
    "                      - 'Error rate': Proportion of incorrect predictions.\n",
    "                      - 'Sensitivity (Recall)': True positive rate for each class.\n",
    "                      - 'Specificity': True negative rate for each class.\n",
    "                      - 'Precision': Proportion of positive identifications that were actually correct for each class.\n",
    "                      - 'F1': Harmonic mean of precision and recall for each class.\n",
    "                      - 'F2': Weighted harmonic mean of precision and recall with more weight on recall for each class.\n",
    "                      - 'F0.5': Weighted harmonic mean of precision and recall with more weight on precision for each class.\n",
    "\n",
    "    Example:\n",
    "    >>> from sklearn.metrics import confusion_matrix\n",
    "    >>> import numpy as np\n",
    "    >>> cm = np.array([[50, 10, 5],\n",
    "                       [5, 35, 5],\n",
    "                       [5, 10, 40]])\n",
    "    >>> cm_performance(cm)\n",
    "\n",
    "    Docstring generated by ChatGPT\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to hold metrics for each class\n",
    "    classes = cm.shape[0]\n",
    "    metrics = ['Accuracy', 'Error rate', 'Sensitivity (Recall)', \n",
    "               'Specificity', 'Precision', 'F1', 'F2', 'F0.5']\n",
    "    performance_dict = {metric: [] for metric in metrics}\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    for i in range(classes):\n",
    "        TP = cm[i, i]\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        TN = np.sum(cm) - (TP + FP + FN)\n",
    "\n",
    "        accuracy = (TP + TN) / np.sum(cm)\n",
    "        error_rate = 1 - accuracy\n",
    "        sensitivity_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        f1 = (2 * precision * sensitivity_recall) / (precision + sensitivity_recall) if (precision + sensitivity_recall) != 0 else 0\n",
    "        f2 = (5 * precision * sensitivity_recall) / ((4 * precision) + sensitivity_recall) if ((4 * precision) + sensitivity_recall) != 0 else 0\n",
    "        f05 = (1.25 * precision * sensitivity_recall) / ((0.25 * precision) + sensitivity_recall) if ((0.25 * precision) + sensitivity_recall) != 0 else 0\n",
    "\n",
    "        performance_dict['Accuracy'].append(accuracy)\n",
    "        performance_dict['Error rate'].append(error_rate)\n",
    "        performance_dict['Sensitivity (Recall)'].append(sensitivity_recall)\n",
    "        performance_dict['Specificity'].append(specificity)\n",
    "        performance_dict['Precision'].append(precision)\n",
    "        performance_dict['F1'].append(f1)\n",
    "        performance_dict['F2'].append(f2)\n",
    "        performance_dict['F0.5'].append(f05)\n",
    "\n",
    "    # Convert to DataFrame with classes as columns\n",
    "    performance_df = pd.DataFrame(performance_dict, \n",
    "                                  index=[f'Class {i}' for i in range(classes)])\n",
    "    performance_df = performance_df.T.round(rnd)  # Transpose and round\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
